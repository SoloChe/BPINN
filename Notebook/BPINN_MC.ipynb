{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import scipy.io\n",
    "from utils import log_gaussian_loss, gaussian, get_kl_Gaussian_divergence\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        n_hidden = len(layers) \n",
    "        self.activation = nn.Tanh\n",
    "        self.dropout = nn.Dropout\n",
    "        layer_list = []\n",
    "\n",
    "        for i in range(n_hidden-2):\n",
    "\n",
    "            linear = torch.nn.Linear(layers[i], layers[i+1])\n",
    "\n",
    "            nn.init.xavier_normal_(linear.weight.data, gain=1.0)\n",
    "            nn.init.zeros_(linear.bias.data)\n",
    "\n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, linear)\n",
    "            )\n",
    "            layer_list.append(\n",
    "                ('activation_%d' % i, self.activation())\n",
    "            )\n",
    "\n",
    "            layer_list.append(\n",
    "                ('dropout_%d' % i, self.dropout(p = 0.2))\n",
    "            )\n",
    "        \n",
    "        linear = torch.nn.Linear(layers[n_hidden-2], layers[n_hidden-1])\n",
    "        nn.init.xavier_normal_(linear.weight.data, gain=1.0)\n",
    "        nn.init.zeros_(linear.bias.data)\n",
    "\n",
    "        layer_list.append(('layer_%d' % (n_hidden-2), linear))\n",
    "        \n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Burgers:\n",
    "    \n",
    "    def __init__(self, net, Xt, Y, lb, ub, device):\n",
    "        \n",
    "        # data\n",
    "        self.X = Xt[:,0:1]; self.t = Xt[:,1:2]\n",
    "        self.Y = torch.tensor(Y).float().to(device)\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "\n",
    "        # net\n",
    "        self.net = net.to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.tensor([0.0], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.tensor([-6.0], requires_grad=True).to(device)\n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        self.net.register_parameter('lambda_1', self.lambda_1)\n",
    "        self.net.register_parameter('lambda_2', self.lambda_2)\n",
    "\n",
    "       \n",
    "        self.iter = 0\n",
    "        self.opt_Adam =  torch.optim.AdamW(self.net.parameters(), lr = 0.003)\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.opt_Adam, max_lr = 1e-2, \n",
    "                                            steps_per_epoch = 1, epochs = 10000)\n",
    "\n",
    "    def net_U(self, x, t):\n",
    "        xt = torch.cat((x,t), dim=1)\n",
    "        xt = 2*(xt-self.lb)/(self.ub-self.lb) - 1\n",
    "        u = self.net(xt)\n",
    "        return u\n",
    "\n",
    "    def net_F(self, x, t):\n",
    "        lambda_1 = self.lambda_1        \n",
    "        lambda_2 = torch.exp(self.lambda_2)\n",
    "\n",
    "        u = self.net_U(x, t)\n",
    "\n",
    "        u_t = torch.autograd.grad(u, t, torch.ones_like(u),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, torch.ones_like(u), \n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "      \n",
    "        F = u_t + lambda_1*u*u_x - lambda_2*u_xx\n",
    "        \n",
    "        return F\n",
    "\n",
    "    def _train_step(self):\n",
    "        \n",
    "        X = torch.tensor(self.X, requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(self.t, requires_grad=True).float().to(device)\n",
    "        \n",
    "        u_pred = self.net_U(X, t)\n",
    "        f_pred = self.net_F(X, t)\n",
    "        loss = torch.mean((self.Y-u_pred)**2) + torch.mean(f_pred**2)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _closure(self):\n",
    "        loss = self._train_step()\n",
    "        self.opt_LBFGS.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        self.iter += 1\n",
    "        if self.iter % 200 == 0:\n",
    "            print('epoch: {}  loss: {:.3e}  lam_1: {:4f}  lam_2: {:.6f}'.format(self.iter, \n",
    "                                                                                loss.item(),\n",
    "                                                                                self.lambda_1.item(),\n",
    "                                                                                torch.exp(self.lambda_2).item()))\n",
    "        return loss\n",
    "\n",
    "    def train(self, epochs):\n",
    "        self.net.train() # training mode\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            loss = self._train_step() \n",
    "            \n",
    "            self.opt_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.opt_Adam.step()\n",
    "\n",
    "            if (epoch+1) % 200 == 0:\n",
    "                print('epoch: {}  loss: {:.3e}  lam_1: {:4f}  lam_2: {:.6f}'.format(epoch+1, \n",
    "                                                                                    loss.item(),\n",
    "                                                                                    self.lambda_1.item(),\n",
    "                                                                                    torch.exp(self.lambda_2).item()))\n",
    "     \n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        x = torch.tensor(x, requires_grad=True).float().to(device)\n",
    "    \n",
    "        self.net.eval()\n",
    "       \n",
    "        u_pred = self.net_U(x[:,0:1], x[:,1:2])\n",
    "        \n",
    "        return u_pred.detach().cpu().numpy()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_u = 2000\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "import scipy.io\n",
    "data = scipy.io.loadmat('./Data/burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None] # 100 x 1\n",
    "x = data['x'].flatten()[:,None] # 256 x 1\n",
    "Exact = np.real(data['usol']).T # 100 x 256\n",
    "\n",
    "X, T = np.meshgrid(x,t) # 100 x 256\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # 25600 x 2\n",
    "u_star = Exact.flatten()[:,None] # 25600 x 1         \n",
    "\n",
    "# Domain bounds of x, t\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)   \n",
    "\n",
    "# training data\n",
    "noise = 0.0            \n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (layers): Sequential(\n",
       "    (layer_0): Linear(in_features=2, out_features=20, bias=True)\n",
       "    (activation_0): Tanh()\n",
       "    (dropout_0): Dropout(p=0.2, inplace=False)\n",
       "    (layer_1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_1): Tanh()\n",
       "    (dropout_1): Dropout(p=0.2, inplace=False)\n",
       "    (layer_2): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_2): Tanh()\n",
       "    (dropout_2): Dropout(p=0.2, inplace=False)\n",
       "    (layer_3): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_3): Tanh()\n",
       "    (dropout_3): Dropout(p=0.2, inplace=False)\n",
       "    (layer_4): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_4): Tanh()\n",
       "    (dropout_4): Dropout(p=0.2, inplace=False)\n",
       "    (layer_5): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_5): Tanh()\n",
       "    (dropout_5): Dropout(p=0.2, inplace=False)\n",
       "    (layer_6): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_6): Tanh()\n",
       "    (dropout_6): Dropout(p=0.2, inplace=False)\n",
       "    (layer_7): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (activation_7): Tanh()\n",
       "    (dropout_7): Dropout(p=0.2, inplace=False)\n",
       "    (layer_8): Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DNN(layers)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PINN_Burgers' object has no attribute 'opt_LBFGS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pinn \u001b[39m=\u001b[39m PINN_Burgers(net, X_u_train, u_train, lb, ub, device)\n\u001b[0;32m----> 2\u001b[0m pinn\u001b[39m.\u001b[39;49mtrain(\u001b[39m0\u001b[39;49m) \n",
      "Cell \u001b[0;32mIn[23], line 92\u001b[0m, in \u001b[0;36mPINN_Burgers.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m200\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     88\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m  loss: \u001b[39m\u001b[39m{:.3e}\u001b[39;00m\u001b[39m  lam_1: \u001b[39m\u001b[39m{:4f}\u001b[39;00m\u001b[39m  lam_2: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[1;32m     89\u001b[0m                                                                             loss\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m     90\u001b[0m                                                                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_1\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m     91\u001b[0m                                                                             torch\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_2)\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m---> 92\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopt_LBFGS\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closure)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PINN_Burgers' object has no attribute 'opt_LBFGS'"
     ]
    }
   ],
   "source": [
    "pinn = PINN_Burgers(net, X_u_train, u_train, lb, ub, device)\n",
    "pinn.train(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca69f467ffdc3dc00e55b12e085102ec88652c053799da0c2cca2d561c3b19a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
