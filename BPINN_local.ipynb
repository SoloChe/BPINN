{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import scipy.io\n",
    "from utils import log_gaussian_loss, gaussian, get_kl_Gaussian_divergence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device: {}'.format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesLinear_local_reparam(nn.Module):\n",
    "    \"\"\"Linear Layer where activations are sampled from a fully factorised normal which is given by aggregating\n",
    "     the moments of each weight's normal distribution. The KL divergence is obtained in closed form. Only works\n",
    "      with gaussian priors.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out, prior):\n",
    "        super(BayesLinear_local_reparam, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.prior = prior\n",
    "\n",
    "        # Learnable parameters\n",
    "        self.W_mu = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-0.05, 0.05))\n",
    "        self.W_p = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-3, -2))\n",
    "        self.b_mu = nn.Parameter(torch.Tensor(self.n_out).uniform_(-0.05, 0.05))\n",
    "        self.b_p = nn.Parameter(torch.Tensor(self.n_out).uniform_(-3, -2))\n",
    "\n",
    "    def forward(self, X, sample=False):\n",
    "        #         print(self.training)\n",
    "\n",
    "        if not self.training and not sample:  # This is just a placeholder function\n",
    "            output = torch.mm(X, self.W_mu) + self.b_mu.expand(X.size()[0], self.n_out)\n",
    "            return output, 0, 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            # calculate std\n",
    "            std_w = 1e-6 + F.softplus(self.W_p, beta=1, threshold=20)\n",
    "            std_b = 1e-6 + F.softplus(self.b_p, beta=1, threshold=20)\n",
    "\n",
    "            act_W_mu = torch.mm(X, self.W_mu)  # self.W_mu + std_w * eps_Ws\n",
    "            act_W_std = torch.sqrt(torch.mm(X.pow(2), std_w.pow(2)))\n",
    "\n",
    "            eps_W = self.W_mu.data.new(act_W_std.size()).normal_(mean=0, std=1)\n",
    "            eps_b = self.b_mu.data.new(std_b.size()).normal_(mean=0, std=1)\n",
    "\n",
    "            act_W_out = act_W_mu + act_W_std * eps_W  # (batch_size, n_output)\n",
    "            act_b_out = self.b_mu + std_b * eps_b\n",
    "\n",
    "            output = act_W_out + act_b_out\n",
    "\n",
    "            # kld = KLD_cost(mu_p=0, sig_p=self.prior_sig, mu_q=self.W_mu, sig_q=std_w) + \\\n",
    "            #  KLD_cost(mu_p=0, sig_p=0.1, mu_q=self.b_mu, sig_q=std_b)\n",
    "\n",
    "            KL_loss_weight = get_kl_Gaussian_divergence(self.prior.mu, self.prior.sigma**2, self.W_mu, std_w**2)\n",
    "            KL_loss_bias = get_kl_Gaussian_divergence(self.prior.mu, self.prior.sigma**2, self.b_mu, std_b**2)\n",
    "            KL_loss = KL_loss_weight + KL_loss_bias\n",
    "            return output, KL_loss, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bayes_linear_LR(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_hid, prior_sig):\n",
    "        super(bayes_linear_LR, self).__init__()\n",
    "\n",
    "        self.prior_sig = prior_sig\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.bfc1 = BayesLinear_local_reparam(input_dim, n_hid, self.prior_sig)\n",
    "        self.bfc2 = BayesLinear_local_reparam(n_hid, n_hid, self.prior_sig)\n",
    "        self.bfc3 = BayesLinear_local_reparam(n_hid, n_hid, self.prior_sig)\n",
    "        self.bfc4 = BayesLinear_local_reparam(n_hid, n_hid, self.prior_sig)\n",
    "        self.bfc5 = BayesLinear_local_reparam(n_hid, output_dim, self.prior_sig)\n",
    "\n",
    "      \n",
    "        self.act = nn.Tanh()\n",
    "        \n",
    "\n",
    "    def forward(self, x, sample=False):\n",
    "        tlqw = 0\n",
    "        tlpw = 0\n",
    "\n",
    "        # -----------------\n",
    "        x, lqw, lpw = self.bfc1(x, sample)\n",
    "        tlqw += lqw\n",
    "        tlpw += lpw\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        x, lqw, lpw = self.bfc2(x, sample)\n",
    "        tlqw += lqw\n",
    "        tlpw += lpw\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        y, lqw, lpw = self.bfc3(x, sample)\n",
    "        tlqw += lqw\n",
    "        tlpw += lpw\n",
    "        # -----------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        y, lqw, lpw = self.bfc4(x, sample)\n",
    "        tlqw += lqw\n",
    "        tlpw += lpw\n",
    "        #------------------\n",
    "        x = self.act(x)\n",
    "        # -----------------\n",
    "        y, lqw, lpw = self.bfc5(x, sample)\n",
    "        tlqw += lqw\n",
    "        tlpw += lpw\n",
    "\n",
    "        return y, tlqw, tlpw\n",
    "\n",
    "    def sample_predict(self, x, N_samples):\n",
    "        # Just copies type from x, initializes new vector\n",
    "        predictions = x.data.new(N_samples, x.shape[0], self.output_dim)\n",
    "        tlqw_vec = np.zeros(N_samples)\n",
    "        tlpw_vec = np.zeros(N_samples)\n",
    "\n",
    "        for i in range(N_samples):\n",
    "            y, tlqw, tlpw = self.forward(x, sample=True)\n",
    "            predictions[i] = y\n",
    "            tlqw_vec[i] = tlqw\n",
    "            tlpw_vec[i] = tlpw\n",
    "\n",
    "        return predictions, tlqw_vec, tlpw_vec\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBP_Model_PINN_LR:\n",
    "    def __init__(self, xt_lb, xt_ub, u_lb, u_ub,\n",
    "                 input_dim, output_dim, no_units,\n",
    "                 learn_rate, batch_size, no_batches,\n",
    "                 prior_lambda1, prior_lambda2, num_epochs):\n",
    "\n",
    "        \n",
    "        self.xt_lb = torch.from_numpy(xt_lb).float().to(device)\n",
    "        self.xt_ub = torch.from_numpy(xt_ub).float().to(device)\n",
    "        self.u_lb = torch.from_numpy(u_lb).float().to(device)\n",
    "        self.u_ub = torch.from_numpy(u_ub).float().to(device)\n",
    "\n",
    "\n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.no_batches = no_batches\n",
    "\n",
    "        self.network = bayes_linear_LR(input_dim = input_dim, output_dim = output_dim,\n",
    "                                        n_hid = no_units, prior_sig = gaussian(0, 1))\n",
    "\n",
    "        # two PDE parameters\n",
    "        self.log_noise_u = torch.log(torch.tensor([0.01], device = device))\n",
    "        self.log_noise_f = torch.log(torch.tensor([0.01], device = device))\n",
    "        # self.network.register_parameter('log_noise_u', self.log_noise_u)\n",
    "        # self.network.register_parameter('log_noise_f', self.log_noise_f)\n",
    "\n",
    "        self.prior_lambda1 = prior_lambda1\n",
    "        self.prior_lambda2 = prior_lambda2\n",
    "\n",
    "        self.lambda1_mus = nn.Parameter(torch.Tensor(1).uniform_(0, 2))\n",
    "        self.lambda1_rhos = nn.Parameter(torch.Tensor(1).uniform_(-3, 2))\n",
    "        self.lambda2_mus = nn.Parameter(torch.Tensor(1).uniform_(0, 0.05))\n",
    "        self.lambda2_rhos = nn.Parameter(torch.Tensor(1).uniform_(-3, -2))\n",
    "\n",
    "        self.network.register_parameter('lambda1_mu', self.lambda1_mus)\n",
    "        self.network.register_parameter('lambda2_mu', self.lambda2_mus)\n",
    "        self.network.register_parameter('lambda1_rho', self.lambda1_rhos)\n",
    "        self.network.register_parameter('lambda2_rho', self.lambda2_rhos)\n",
    "\n",
    "        self.network = self.network.to(device)\n",
    "\n",
    "        # self.optimizer = torch.optim.SGD(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.optimizer = torch.optim.AdamW(self.network.parameters(), lr = self.learn_rate)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr = 1e-2, \n",
    "        #                                     steps_per_epoch = no_batches, epochs = num_epochs)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "\n",
    "    def net_U(self, x, t):\n",
    "        xt = torch.cat((x,t), dim=1)\n",
    "        xt = 2*(xt-self.xt_lb)/(self.xt_ub-self.xt_lb) - 1\n",
    "        u, KL_loss, _ = self.network(xt)\n",
    "        return u, KL_loss\n",
    "\n",
    "    def net_F(self, x, t, lambda1_sample, lambda2_sample):\n",
    "        lambda_1 = lambda1_sample        \n",
    "        lambda_2 = torch.exp(lambda2_sample)\n",
    "\n",
    "        u, _ = self.net_U(x, t)\n",
    "        u = u*(self.u_ub-self.u_lb) + self.u_lb # reverse scaling\n",
    "\n",
    "        u_t = torch.autograd.grad(u, t, torch.ones_like(u),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, torch.ones_like(u),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "\n",
    "        # F = u_t + 1*u*u_x - 0.003*u_xx\n",
    "        F = u_t + lambda_1*u*u_x - lambda_2*u_xx\n",
    "\n",
    "        return F\n",
    "\n",
    "    def fit(self, X, t, U, no_samples):\n",
    "        self.network.train()\n",
    "\n",
    "        U = (U-self.u_lb)/(self.u_ub-self.u_lb) # scaling\n",
    "\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        fit_loss_total = 0\n",
    "        fit_loss_F_total = 0\n",
    "        fit_loss_U_total = 0\n",
    "\n",
    "        for i in range(no_samples):\n",
    "            lambda1_epsilons = self.lambda1_mus.data.new(self.lambda1_mus.size()).normal_()\n",
    "            lambda1_stds = torch.log(1 + torch.exp(self.lambda1_rhos))\n",
    "            lambda2_epsilons = self.lambda2_mus.data.new(self.lambda2_mus.size()).normal_()\n",
    "            lambda2_stds = torch.log(1 + torch.exp(self.lambda2_rhos))\n",
    "\n",
    "            lambda1_sample = self.lambda1_mus + lambda1_epsilons * lambda1_stds\n",
    "            lambda2_sample = self.lambda2_mus + lambda2_epsilons * lambda2_stds\n",
    "\n",
    "            u_pred, KL_loss_para = self.net_U(X, t)\n",
    "            f_pred = self.net_F(X, t, lambda1_sample, lambda2_sample)\n",
    "            \n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss_U_total += self.loss_func(u_pred, U, self.log_noise_u.exp(), self.network.output_dim)\n",
    "            fit_loss_F_total += self.loss_func(f_pred, torch.zeros_like(f_pred), self.log_noise_f.exp(), self.network.output_dim)\n",
    "            \n",
    "\n",
    "        KL_loss_lambda1 = get_kl_Gaussian_divergence(self.prior_lambda1.mu, self.prior_lambda1.sigma**2, self.lambda1_mus, lambda1_stds**2)\n",
    "        KL_loss_lambda2 = get_kl_Gaussian_divergence(self.prior_lambda2.mu, self.prior_lambda2.sigma**2, self.lambda2_mus, lambda2_stds**2)\n",
    "        KL_loss_total = KL_loss_para + KL_loss_lambda1 + KL_loss_lambda2\n",
    "\n",
    "        # minibatches and KL reweighting\n",
    "        KL_loss_total = KL_loss_total/self.no_batches\n",
    "        # expected loglikelihood\n",
    "        total_loss = (KL_loss_total + fit_loss_U_total + fit_loss_F_total) / (no_samples*X.shape[0])\n",
    "\n",
    "        \n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # self.scheduler.step()\n",
    "\n",
    "        return fit_loss_U_total/no_samples, fit_loss_F_total/no_samples, KL_loss_total, total_loss\n",
    "\n",
    "    def predict(self, xt, no_sample, best_net):\n",
    "        xt = torch.tensor(xt, requires_grad=True).float().to(device)\n",
    "        self.network.eval()\n",
    "        sample = []\n",
    "      \n",
    "        for i in range(no_sample):\n",
    "            \n",
    "            xt = 2*(xt-self.xt_lb)/(self.xt_ub-self.xt_lb) - 1\n",
    "            u_pred, _, _ = best_net(xt)\n",
    "            u_pred = u_pred*(self.u_ub-self.u_lb) + self.u_lb # reverse scaling\n",
    "            sample.append(u_pred.detach().cpu().numpy())\n",
    "        return np.array(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('./Data/burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None] # 100 x 1\n",
    "x = data['x'].flatten()[:,None] # 256 x 1\n",
    "Exact = np.real(data['usol']).T # 100 x 256\n",
    "\n",
    "X, T = np.meshgrid(x,t) # 100 x 256\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # 25600 x 2\n",
    "u_star = Exact.flatten()[:,None] # 25600 x 1\n",
    "\n",
    "# Domain bounds of x, t\n",
    "xt_lb = X_star.min(0)\n",
    "xt_ub = X_star.max(0)\n",
    "\n",
    "# training data\n",
    "N_u = 2000\n",
    "noise = 0.0\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "u_lb = u_train.min(0)\n",
    "u_ub = u_train.max(0)\n",
    "\n",
    "num_epochs, batch_size = 10000, len(X_u_train),\n",
    "\n",
    "net = BBP_Model_PINN_LR(xt_lb, xt_ub, u_lb, u_ub,\n",
    "                        input_dim = 2, output_dim = 1, no_units = 50, learn_rate = 1e-3,\n",
    "                           batch_size = batch_size, no_batches = 1,\n",
    "                           prior_lambda1=gaussian(0, 1), prior_lambda2=gaussian(0, 1), num_epochs = num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_loss_U_train = np.zeros(num_epochs)\n",
    "fit_loss_F_train = np.zeros(num_epochs)\n",
    "KL_loss_train = np.zeros(num_epochs)\n",
    "loss = np.zeros(num_epochs)\n",
    "\n",
    "best_net, best_loss = None, float('inf')\n",
    "\n",
    "X = torch.tensor(X_u_train[:,0:1], requires_grad = True, device = device).float()\n",
    "t = torch.tensor(X_u_train[:,1:2], requires_grad = True, device = device).float()\n",
    "U = torch.tensor(u_train, requires_grad = True, device = device).float()\n",
    "\n",
    "X_u_test_25 = np.hstack([x, 0.25*np.ones_like((x))])\n",
    "target_25 = Exact[25].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     1/10000, total loss = 1343653.000, Fit loss U = 19905154.000, Fit loss F = 2667400192.000, KL loss = 17327.654\n",
      "Epoch:     1/10000, lambda1_mu = 0.941, lambda2_mu = 1.030, lambda1_std = 0.530, lambda2_std = 0.124\n",
      "Epoch:     1/10000, error_25 = 4.11953, error_train = 4.70155\n",
      "Epoch:     1/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n",
      "Epoch:   101/10000, total loss = 88789.672, Fit loss U = 2465759.250, Fit loss F = 175112752.000, KL loss = 16652.473\n",
      "Epoch:   101/10000, lambda1_mu = 0.918, lambda2_mu = 0.974, lambda1_std = 0.520, lambda2_std = 0.120\n",
      "Epoch:   101/10000, error_25 = 2.63963, error_train = 2.95371\n",
      "Epoch:   101/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n",
      "Epoch:   201/10000, total loss = 44031.957, Fit loss U = 1992445.875, Fit loss F = 86070672.000, KL loss = 16056.531\n",
      "Epoch:   201/10000, lambda1_mu = 0.912, lambda2_mu = 0.953, lambda1_std = 0.517, lambda2_std = 0.119\n",
      "Epoch:   201/10000, error_25 = 1.15260, error_train = 1.29181\n",
      "Epoch:   201/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n",
      "Epoch:   301/10000, total loss = 23921.902, Fit loss U = 2214826.750, Fit loss F = 45628204.000, KL loss = 15491.524\n",
      "Epoch:   301/10000, lambda1_mu = 0.908, lambda2_mu = 0.941, lambda1_std = 0.516, lambda2_std = 0.118\n",
      "Epoch:   301/10000, error_25 = 1.05896, error_train = 1.19760\n",
      "Epoch:   301/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n",
      "Epoch:   401/10000, total loss = 15975.199, Fit loss U = 1607348.250, Fit loss F = 30342304.000, KL loss = 14953.964\n",
      "Epoch:   401/10000, lambda1_mu = 0.905, lambda2_mu = 0.932, lambda1_std = 0.514, lambda2_std = 0.118\n",
      "Epoch:   401/10000, error_25 = 0.99506, error_train = 1.14671\n",
      "Epoch:   401/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n",
      "Epoch:   501/10000, total loss = 12153.972, Fit loss U = 1683167.375, Fit loss F = 22624054.000, KL loss = 14440.583\n",
      "Epoch:   501/10000, lambda1_mu = 0.902, lambda2_mu = 0.926, lambda1_std = 0.513, lambda2_std = 0.118\n",
      "Epoch:   501/10000, error_25 = 1.71090, error_train = 2.00913\n",
      "Epoch:   501/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n",
      "Epoch:   601/10000, total loss = 8308.640, Fit loss U = 1526545.000, Fit loss F = 15090039.000, KL loss = 13947.065\n",
      "Epoch:   601/10000, lambda1_mu = 0.899, lambda2_mu = 0.922, lambda1_std = 0.513, lambda2_std = 0.118\n",
      "Epoch:   601/10000, error_25 = 1.67991, error_train = 1.98722\n",
      "Epoch:   601/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n",
      "Epoch:   701/10000, total loss = 6493.823, Fit loss U = 1275883.875, Fit loss F = 11711090.000, KL loss = 13471.527\n",
      "Epoch:   701/10000, lambda1_mu = 0.897, lambda2_mu = 0.918, lambda1_std = 0.512, lambda2_std = 0.118\n",
      "Epoch:   701/10000, error_25 = 1.67686, error_train = 1.99075\n",
      "Epoch:   701/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n",
      "Epoch:   801/10000, total loss = 4696.834, Fit loss U = 1281929.625, Fit loss F = 8111089.000, KL loss = 13012.034\n",
      "Epoch:   801/10000, lambda1_mu = 0.895, lambda2_mu = 0.915, lambda1_std = 0.512, lambda2_std = 0.118\n",
      "Epoch:   801/10000, error_25 = 1.63499, error_train = 1.95233\n",
      "Epoch:   801/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n",
      "Epoch:   901/10000, total loss = 4250.106, Fit loss U = 1102476.750, Fit loss F = 7397107.500, KL loss = 12567.241\n",
      "Epoch:   901/10000, lambda1_mu = 0.894, lambda2_mu = 0.913, lambda1_std = 0.512, lambda2_std = 0.118\n",
      "Epoch:   901/10000, error_25 = 1.49042, error_train = 1.81288\n",
      "Epoch:   901/10000, noise_f = 0.01000, noise_u = 0.01000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment = '_local_vi')\n",
    "\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    EU, EF, KL_loss, total_loss = net.fit(X, t, U, no_samples = 20)\n",
    "    \n",
    "    fit_loss_U_train[i] = EU.item()\n",
    "    fit_loss_F_train[i] = EF.item()\n",
    "    KL_loss_train[i] = KL_loss.item()\n",
    "    loss[i] = total_loss.item()\n",
    "\n",
    "    writer.add_scalar(\"loss/total_loss\", loss[i], i)\n",
    "    writer.add_scalar(\"loss/U_loss\", fit_loss_U_train[i], i)\n",
    "    writer.add_scalar(\"loss/F_loss\", fit_loss_F_train[i], i)\n",
    "    writer.add_scalar(\"loss/KL_loss\", KL_loss_train[i], i)\n",
    "    \n",
    "\n",
    "    if fit_loss_U_train[i] + fit_loss_F_train[i] < best_loss:\n",
    "        best_loss = fit_loss_U_train[i] + fit_loss_F_train[i]\n",
    "        best_net = copy.deepcopy(net.network)\n",
    "\n",
    "    if i % 100 == 0 or i == num_epochs - 1:\n",
    "\n",
    "        print(\"Epoch: {:5d}/{:5d}, total loss = {:.3f}, Fit loss U = {:.3f}, Fit loss F = {:.3f}, KL loss = {:.3f}\".format(i + 1, num_epochs, \n",
    "               loss[i], fit_loss_U_train[i], fit_loss_F_train[i], KL_loss_train[i]))\n",
    "\n",
    "    \n",
    "        lambda1_mus = net.lambda1_mus.item()\n",
    "        lambda1_stds = torch.log(1 + torch.exp(net.lambda1_rhos)).item()\n",
    "        \n",
    "        lambda2_mus = np.exp(net.lambda2_mus.item())\n",
    "        lambda2_stds = torch.log(1 + torch.exp(net.lambda2_rhos)).item()\n",
    "\n",
    "        noise_f = net.log_noise_u.exp().item()\n",
    "        noise_u = net.log_noise_u.exp().item()\n",
    "        \n",
    "        samples_25 = net.predict(X_u_test_25, 100, net.network)\n",
    "        u_pred_25 = samples_25.mean(axis=0)\n",
    "        error_25 = np.linalg.norm(target_25-u_pred_25, 2)/np.linalg.norm(target_25, 2)\n",
    "\n",
    "        samples_train = net.predict(X_u_train, 100, net.network)\n",
    "        u_pred_train = samples_train.mean(axis=0)\n",
    "        error_train = np.linalg.norm(u_train-u_pred_train, 2)/np.linalg.norm(u_train, 2)\n",
    "\n",
    "\n",
    "        # writer.add_scalars(\"loss/train_test\", {'train':error_train, 'test':error_25}, i)\n",
    "        # writer.add_scalars(\"loss/f_u\", {'noise_f':noise_f, 'noise_u':noise_u}, i)\n",
    "       \n",
    "        print(\"Epoch: {:5d}/{:5d}, lambda1_mu = {:.3f}, lambda2_mu = {:.3f}, lambda1_std = {:.3f}, lambda2_std = {:.3f}\".format(i + 1, num_epochs,\n",
    "                                                                                                                        lambda1_mus, lambda2_mus,\n",
    "                                                                                                                        lambda1_stds, lambda2_stds))\n",
    "        print(\"Epoch: {:5d}/{:5d}, error_25 = {:.5f}, error_train = {:.5f}\".format(i+1, num_epochs, error_25, error_train))\n",
    "        print(\"Epoch: {:5d}/{:5d}, noise_f = {:.5f}, noise_u = {:.5f}\".format(i+1, num_epochs, noise_f, noise_u))\n",
    "        print()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_test_25 = np.hstack([x, 0.25*np.ones_like((x))]); u_test_25 = Exact[25]\n",
    "X_u_test_50 = np.hstack([x, 0.50*np.ones_like((x))]); u_test_50 = Exact[50]\n",
    "X_u_test_75 = np.hstack([x, 0.75*np.ones_like((x))]); u_test_75 = Exact[75]\n",
    "\n",
    "\n",
    "\n",
    "# aleatoric = best_net.log_noise.exp().cpu().data.numpy()\n",
    "# epistemic = samples_25.var(axis = 0)**0.5\n",
    "# total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
    "\n",
    "samples_25 = net.predict(X_u_test_25, 100, net.network)\n",
    "samples_50 = net.predict(X_u_test_50, 100, net.network)\n",
    "samples_75 = net.predict(X_u_test_75, 100, net.network)\n",
    "\n",
    "\n",
    "u_pred_25 = samples_25.mean(axis = 0)\n",
    "u_pred_50 = samples_50.mean(axis = 0)\n",
    "u_pred_75 = samples_75.mean(axis = 0)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (15,4))\n",
    "axs[0].plot(x,u_test_25, 'b-', linewidth = 2, label = 'Exact')\n",
    "axs[0].plot(x,u_pred_25, 'r--', linewidth = 2, label = 'Prediction')\n",
    "axs[0].set_xlabel('$x$')\n",
    "axs[0].set_ylabel('$u(t,x)$')\n",
    "axs[0].set_title('$t = 0.25$', fontsize = 10)\n",
    "axs[0].axis('square')\n",
    "#ax.set_xlim([-1.1,1.1])\n",
    "#ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "\n",
    "axs[1].plot(x,u_test_50, 'b-', linewidth = 2, label = 'Exact')\n",
    "axs[1].plot(x,u_pred_50, 'r--', linewidth = 2, label = 'Prediction')\n",
    "axs[1].set_xlabel('$x$')\n",
    "axs[1].set_ylabel('$u(t,x)$')\n",
    "axs[1].set_title('$t = 0.5$', fontsize = 10)\n",
    "axs[1].axis('square')\n",
    "\n",
    "\n",
    "axs[2].plot(x,u_test_75, 'b-', linewidth = 2, label = 'Exact')\n",
    "axs[2].plot(x,u_pred_75, 'r--', linewidth = 2, label = 'Prediction')\n",
    "axs[2].set_xlabel('$x$')\n",
    "axs[2].set_ylabel('$u(t,x)$')\n",
    "axs[2].set_title('$t = 0.75$', fontsize = 10)\n",
    "axs[2].axis('square')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca69f467ffdc3dc00e55b12e085102ec88652c053799da0c2cca2d561c3b19a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
