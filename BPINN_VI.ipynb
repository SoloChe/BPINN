{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import scipy.io\n",
    "from utils import log_gaussian_loss, gaussian, get_kl_Gaussian_divergence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BayesLinear_Normalq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, prior):\n",
    "        super(BayesLinear_Normalq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.prior = prior\n",
    "\n",
    "        scale = (2 / self.input_dim) ** 0.5\n",
    "        rho_init = np.log(np.exp((2 / self.input_dim) ** 0.5) - 1)\n",
    "\n",
    "        self.weight_mus = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.05, 0.05))\n",
    "        self.weight_rhos = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-3, -2))\n",
    "\n",
    "        self.bias_mus = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.05, 0.05))\n",
    "        self.bias_rhos = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-3, -2))\n",
    "\n",
    "        # nn.init.xavier_normal_(self.weight_mus.data, gain=1.0)\n",
    "        # nn.init.zeros_(self.bias_mus.data)\n",
    "\n",
    "    def forward(self, x, sample=True):\n",
    "\n",
    "        if sample:\n",
    "            # sample gaussian noise for each weight and each bias\n",
    "            weight_epsilons = self.weight_mus.data.new(self.weight_mus.size()).normal_()\n",
    "            bias_epsilons = self.bias_mus.data.new(self.bias_mus.size()).normal_()\n",
    "\n",
    "            # calculate the weight and bias stds from the rho parameters\n",
    "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "            bias_stds = torch.log(1 + torch.exp(self.bias_rhos))\n",
    "\n",
    "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "            weight_sample = self.weight_mus + weight_epsilons * weight_stds\n",
    "            bias_sample = self.bias_mus + bias_epsilons * bias_stds\n",
    "            output = torch.mm(x, weight_sample) + bias_sample\n",
    "\n",
    "            # computing the KL loss term\n",
    "            KL_loss_weight = get_kl_Gaussian_divergence(self.prior.mu, self.prior.sigma**2, self.weight_mus, weight_stds**2)\n",
    "            KL_loss_bias = get_kl_Gaussian_divergence(self.prior.mu, self.prior.sigma**2, self.bias_mus, bias_stds**2)\n",
    "            KL_loss = KL_loss_weight + KL_loss_bias\n",
    "\n",
    "            return output, KL_loss\n",
    "        else:\n",
    "            output = torch.mm(x, self.weight_mus) + self.bias_mus\n",
    "            return output, KL_loss\n",
    "\n",
    "    def sample_layer(self, no_samples):\n",
    "        all_samples = []\n",
    "        for i in range(no_samples):\n",
    "            # sample gaussian noise for each weight and each bias\n",
    "            weight_epsilons = Variable(self.weight_mus.data.new(self.weight_mus.size()).normal_())\n",
    "\n",
    "            # calculate the weight and bias stds from the rho parameters\n",
    "            weight_stds = torch.log(1 + torch.exp(self.weight_rhos))\n",
    "\n",
    "            # calculate samples from the posterior from the sampled noise and mus/stds\n",
    "            weight_sample = self.weight_mus + weight_epsilons * weight_stds\n",
    "\n",
    "            all_samples += weight_sample.view(-1).cpu().data.numpy().tolist()\n",
    "\n",
    "        return all_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "layer1 = BayesLinear_Normalq(2, 10, gaussian(0, 1))\n",
    "X = torch.tensor(X_u_train, requires_grad=True).float().to(device)\n",
    "a, b = layer1(X)\n",
    "\n",
    "activation = nn.Tanh()\n",
    "\n",
    "activation(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BBP_Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, no_units):\n",
    "        super(BBP_Model, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # network with two hidden and one output layer\n",
    "        self.layer1 = BayesLinear_Normalq(input_dim, no_units, gaussian(0, 1))\n",
    "        self.layer2 = BayesLinear_Normalq(no_units, no_units, gaussian(0, 1))\n",
    "        self.layer3 = BayesLinear_Normalq(no_units, no_units, gaussian(0, 1))\n",
    "        self.layer4 = BayesLinear_Normalq(no_units, no_units, gaussian(0, 1))\n",
    "        self.layer5 = BayesLinear_Normalq(no_units, no_units, gaussian(0, 1))\n",
    "        self.layer6 = BayesLinear_Normalq(no_units, no_units, gaussian(0, 1))\n",
    "        self.layer7 = BayesLinear_Normalq(no_units, output_dim+1, gaussian(0, 1))\n",
    "\n",
    "        # activation to be used between hidden layers\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        KL_loss_total = 0\n",
    "        x = x.view(-1, self.input_dim)\n",
    "\n",
    "        x, KL_loss = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        KL_loss_total += KL_loss\n",
    "\n",
    "        x, KL_loss = self.layer2(x)\n",
    "        x = self.activation(x)\n",
    "        KL_loss_total += KL_loss\n",
    "\n",
    "        x, KL_loss = self.layer3(x)\n",
    "        x = self.activation(x)\n",
    "        KL_loss_total += KL_loss\n",
    "\n",
    "        x, KL_loss = self.layer4(x)\n",
    "        x = self.activation(x)\n",
    "        KL_loss_total += KL_loss\n",
    "\n",
    "        x, KL_loss = self.layer5(x)\n",
    "        x = self.activation(x)\n",
    "        KL_loss_total += KL_loss\n",
    "\n",
    "        x, KL_loss = self.layer6(x)\n",
    "        x = self.activation(x)\n",
    "        KL_loss_total += KL_loss\n",
    "\n",
    "        x, KL_loss = self.layer7(x)\n",
    "        KL_loss_total += KL_loss\n",
    "\n",
    "        return x, KL_loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BBP_Model_PINN:\n",
    "    def __init__(self, xt_lb, xt_ub, u_lb, u_ub,\n",
    "                 input_dim, output_dim, no_units,\n",
    "                 learn_rate, batch_size, no_batches,\n",
    "                 prior_lambda1, prior_lambda2, num_epochs):\n",
    "\n",
    "        \n",
    "        self.xt_lb = torch.from_numpy(xt_lb).float().to(device)\n",
    "        self.xt_ub = torch.from_numpy(xt_ub).float().to(device)\n",
    "        self.u_lb = torch.from_numpy(u_lb).float().to(device)\n",
    "        self.u_ub = torch.from_numpy(u_ub).float().to(device)\n",
    "\n",
    "\n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.no_batches = no_batches\n",
    "\n",
    "        self.network = BBP_Model(input_dim = input_dim, output_dim = output_dim,\n",
    "                                    no_units = no_units)\n",
    "\n",
    "\n",
    "        self.prior_lambda1 = prior_lambda1\n",
    "        self.prior_lambda2 = prior_lambda2\n",
    "\n",
    "        self.lambda1_mus = nn.Parameter(torch.Tensor(1).uniform_(0, 2))\n",
    "        self.lambda1_rhos = nn.Parameter(torch.Tensor(1).uniform_(-3, 2))\n",
    "        self.lambda2_mus = nn.Parameter(torch.Tensor(1).uniform_(0, 0.05))\n",
    "        self.lambda2_rhos = nn.Parameter(torch.Tensor(1).uniform_(-3, -2))\n",
    "\n",
    "        self.network.register_parameter('lambda1_mu', self.lambda1_mus)\n",
    "        self.network.register_parameter('lambda2_mu', self.lambda2_mus)\n",
    "        self.network.register_parameter('lambda1_rho', self.lambda1_rhos)\n",
    "        self.network.register_parameter('lambda2_rho', self.lambda2_rhos)\n",
    "\n",
    "        self.network = self.network.to(device)\n",
    "\n",
    "        # self.optimizer = torch.optim.SGD(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.optimizer = torch.optim.AdamW(self.network.parameters(), lr = self.learn_rate)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr = 1e-3, \n",
    "        #                                     steps_per_epoch = no_batches, epochs = num_epochs)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "\n",
    "    def net_U(self, x, t):\n",
    "        xt = torch.cat((x,t), dim=1)\n",
    "        xt = 2*(xt-self.xt_lb)/(self.xt_ub-self.xt_lb) - 1\n",
    "        out, KL_loss = self.network(xt)\n",
    "\n",
    "        u = out[:,0:1]\n",
    "        log_noise_u = out[:,1:2]\n",
    "        # log_noise_f = out[:,2:3]\n",
    "        return u, log_noise_u, KL_loss\n",
    "\n",
    "    def net_F(self, x, t, lambda1_sample, lambda2_sample):\n",
    "        lambda_1 = lambda1_sample        \n",
    "        lambda_2 = torch.exp(lambda2_sample)\n",
    "\n",
    "        u, _, _ = self.net_U(x, t)\n",
    "        u = u*(self.u_ub-self.u_lb) + self.u_lb # reverse scaling\n",
    "\n",
    "        u_t = torch.autograd.grad(u, t, torch.ones_like(u),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, torch.ones_like(u),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "\n",
    "        # F = u_t + 1*u*u_x - (0.01/np.pi)*u_xx\n",
    "        F = u_t + lambda_1*u*u_x - lambda_2*u_xx\n",
    "        return F\n",
    "\n",
    "    def fit(self, X, t, U, no_samples):\n",
    "        self.network.train()\n",
    "\n",
    "        # X = torch.tensor(self.X, requires_grad=True).float().to(device)\n",
    "        # t = torch.tensor(self.t, requires_grad=True).float().to(device)\n",
    "        U = (U-self.u_lb)/(self.u_ub-self.u_lb) # scaling\n",
    "\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        fit_loss_total = 0\n",
    "        fit_loss_F_total = 0\n",
    "        fit_loss_U_total = 0\n",
    "\n",
    "        for i in range(no_samples):\n",
    "            lambda1_epsilons = self.lambda1_mus.data.new(self.lambda1_mus.size()).normal_()\n",
    "            lambda1_stds = torch.log(1 + torch.exp(self.lambda1_rhos))\n",
    "            lambda2_epsilons = self.lambda2_mus.data.new(self.lambda2_mus.size()).normal_()\n",
    "            lambda2_stds = torch.log(1 + torch.exp(self.lambda2_rhos))\n",
    "\n",
    "            lambda1_sample = self.lambda1_mus + lambda1_epsilons * lambda1_stds\n",
    "            lambda2_sample = self.lambda2_mus + lambda2_epsilons * lambda2_stds\n",
    "\n",
    "            u_pred, log_noise_u, KL_loss_para = self.net_U(X, t)\n",
    "            f_pred = self.net_F(X, t, lambda1_sample, lambda2_sample)\n",
    "\n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss_U_total += self.loss_func(u_pred, U, log_noise_u.exp(), self.network.output_dim)\n",
    "            fit_loss_F_total += torch.sum(f_pred**2)\n",
    "            # fit_loss_F_total += self.loss_func(f_pred, torch.zeros_like(f_pred), self.network.log_noise_f.exp(), self.network.output_dim)\n",
    "\n",
    "        KL_loss_lambda1 = get_kl_Gaussian_divergence(self.prior_lambda1.mu, self.prior_lambda1.sigma**2, self.lambda1_mus, lambda1_stds**2)\n",
    "        KL_loss_lambda2 = get_kl_Gaussian_divergence(self.prior_lambda2.mu, self.prior_lambda2.sigma**2, self.lambda2_mus, lambda2_stds**2)\n",
    "        KL_loss_total = KL_loss_para + KL_loss_lambda1 + KL_loss_lambda2\n",
    "\n",
    "        # KL_loss_total = KL_loss_para \n",
    "        # minibatches and KL reweighting\n",
    "        KL_loss_total = KL_loss_total/self.no_batches\n",
    "        total_loss = (KL_loss_total + fit_loss_U_total + fit_loss_F_total) / (no_samples*X.shape[0])\n",
    "        \n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # self.scheduler.step()\n",
    "\n",
    "        return fit_loss_U_total/no_samples, fit_loss_F_total/no_samples, KL_loss_total, total_loss\n",
    "\n",
    "    def predict(self, xt, no_sample, best_net):\n",
    "        xt = torch.tensor(xt, requires_grad=True).float().to(device)\n",
    "        xt = 2*(xt-self.xt_lb)/(self.xt_ub-self.xt_lb) - 1\n",
    "\n",
    "        self.network.eval()\n",
    "        samples = []\n",
    "        noises = []\n",
    "        for i in range(no_sample):\n",
    "            out_pred, _ = best_net(xt)\n",
    "            u_pred = out_pred[:,0:1]\n",
    "            log_noise_u = out_pred[:,1:2]\n",
    "            u_pred = u_pred*(self.u_ub-self.u_lb) + self.u_lb # reverse scaling\n",
    "            samples.append(u_pred.detach().cpu().numpy())\n",
    "            noises.append(log_noise_u.exp().detach().cpu().numpy())\n",
    "        return np.array(samples), np.array(noises)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('./Data/burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None] # 100 x 1\n",
    "x = data['x'].flatten()[:,None] # 256 x 1\n",
    "Exact = np.real(data['usol']).T # 100 x 256\n",
    "\n",
    "Exact += np.random.normal(0, 0.03, (100, 256))\n",
    "\n",
    "X, T = np.meshgrid(x,t) # 100 x 256\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # 25600 x 2\n",
    "u_star = Exact.flatten()[:,None]  # 25600 x 1\n",
    "\n",
    "# Domain bounds of x, t\n",
    "xt_lb = X_star.min(0)\n",
    "xt_ub = X_star.max(0)\n",
    "\n",
    "# training data\n",
    "N_u = 4000\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "u_lb = u_train.min(0)\n",
    "u_ub = u_train.max(0)\n",
    "\n",
    "num_epochs, batch_size = 25000, len(X_u_train),\n",
    "\n",
    "net = BBP_Model_PINN(xt_lb, xt_ub, u_lb, u_ub,\n",
    "                     input_dim = 2, output_dim = 1, no_units = 50, learn_rate = 1e-3,\n",
    "                        batch_size = batch_size, no_batches = 1,\n",
    "                        prior_lambda1 = gaussian(0, 1), prior_lambda2 = gaussian(0, 1), num_epochs = num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_loss_U_train = np.zeros(num_epochs)\n",
    "fit_loss_F_train = np.zeros(num_epochs)\n",
    "KL_loss_train = np.zeros(num_epochs)\n",
    "loss = np.zeros(num_epochs)\n",
    "noise = []\n",
    "\n",
    "best_net, best_loss = None, float('inf')\n",
    "\n",
    "X = torch.tensor(X_u_train[:,0:1], requires_grad = True, device = device).float()\n",
    "t = torch.tensor(X_u_train[:,1:2], requires_grad = True, device = device).float()\n",
    "U = torch.tensor(u_train, requires_grad = True, device = device).float()\n",
    "\n",
    "X_u_test_25 = np.hstack([x, 0.25*np.ones_like((x))])\n",
    "target_25 = Exact[25].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     1/25000, total loss = 1.398, Fit loss U = 4259.234, Fit loss F = 4.401, KL loss = 26578.098\n",
      "Epoch:     1/25000, lambda1_mu = 1.858, lambda2_mu = 1.002, lambda1_std = 0.416, lambda2_std = 0.073\n",
      "Epoch:     1/25000, error_25 = 1.75094, error_train = 1.99268\n",
      "\n",
      "Epoch:   101/25000, total loss = 0.763, Fit loss U = 1728.856, Fit loss F = 4.353, KL loss = 26364.479\n",
      "Epoch:   101/25000, lambda1_mu = 1.786, lambda2_mu = 0.955, lambda1_std = 0.413, lambda2_std = 0.081\n",
      "Epoch:   101/25000, error_25 = 1.00382, error_train = 0.99855\n",
      "\n",
      "Epoch:   201/25000, total loss = 0.602, Fit loss U = 1090.862, Fit loss F = 4.206, KL loss = 26273.801\n",
      "Epoch:   201/25000, lambda1_mu = 1.733, lambda2_mu = 0.908, lambda1_std = 0.410, lambda2_std = 0.089\n",
      "Epoch:   201/25000, error_25 = 0.98387, error_train = 0.98663\n",
      "\n",
      "Epoch:   301/25000, total loss = 0.567, Fit loss U = 931.119, Fit loss F = 26.070, KL loss = 26206.191\n",
      "Epoch:   301/25000, lambda1_mu = 1.521, lambda2_mu = 0.850, lambda1_std = 0.379, lambda2_std = 0.088\n",
      "Epoch:   301/25000, error_25 = 0.88940, error_train = 0.87090\n",
      "\n",
      "Epoch:   401/25000, total loss = 0.616, Fit loss U = 1017.196, Fit loss F = 137.861, KL loss = 26148.920\n",
      "Epoch:   401/25000, lambda1_mu = 1.358, lambda2_mu = 0.737, lambda1_std = 0.344, lambda2_std = 0.078\n",
      "Epoch:   401/25000, error_25 = 0.85345, error_train = 0.85623\n",
      "\n",
      "Epoch:   501/25000, total loss = 0.491, Fit loss U = 478.720, Fit loss F = 180.246, KL loss = 26098.324\n",
      "Epoch:   501/25000, lambda1_mu = 1.215, lambda2_mu = 0.603, lambda1_std = 0.309, lambda2_std = 0.069\n",
      "Epoch:   501/25000, error_25 = 0.81133, error_train = 0.81685\n",
      "\n",
      "Epoch:   601/25000, total loss = 0.429, Fit loss U = 296.829, Fit loss F = 116.955, KL loss = 26054.814\n",
      "Epoch:   601/25000, lambda1_mu = 1.109, lambda2_mu = 0.497, lambda1_std = 0.280, lambda2_std = 0.062\n",
      "Epoch:   601/25000, error_25 = 0.79789, error_train = 0.80270\n",
      "\n",
      "Epoch:   701/25000, total loss = 0.423, Fit loss U = 271.384, Fit loss F = 118.365, KL loss = 26010.764\n",
      "Epoch:   701/25000, lambda1_mu = 1.042, lambda2_mu = 0.410, lambda1_std = 0.257, lambda2_std = 0.057\n",
      "Epoch:   701/25000, error_25 = 0.76386, error_train = 0.78506\n",
      "\n",
      "Epoch:   801/25000, total loss = 0.391, Fit loss U = 158.386, Fit loss F = 108.129, KL loss = 25968.656\n",
      "Epoch:   801/25000, lambda1_mu = 1.020, lambda2_mu = 0.338, lambda1_std = 0.238, lambda2_std = 0.053\n",
      "Epoch:   801/25000, error_25 = 0.72646, error_train = 0.76185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment = '_test3_with_singlevar')\n",
    "\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    EU, EF, KL_loss, total_loss = net.fit(X, t, U, no_samples = 20)\n",
    "    \n",
    "    fit_loss_U_train[i] = EU.item()\n",
    "    fit_loss_F_train[i] = EF.item()\n",
    "    KL_loss_train[i] = KL_loss.item()\n",
    "    loss[i] = total_loss.item()\n",
    "\n",
    "    writer.add_scalar(\"loss/total_loss\", loss[i], i)\n",
    "    writer.add_scalar(\"loss/U_loss\", fit_loss_U_train[i], i)\n",
    "    writer.add_scalar(\"loss/F_loss\", fit_loss_F_train[i], i)\n",
    "    writer.add_scalar(\"loss/KL_loss\", KL_loss_train[i], i)\n",
    "    \n",
    "\n",
    "    if fit_loss_U_train[i] + fit_loss_F_train[i] < best_loss:\n",
    "        best_loss = fit_loss_U_train[i] + fit_loss_F_train[i]\n",
    "        best_net = copy.deepcopy(net.network)\n",
    "\n",
    "    if i % 100 == 0 or i == num_epochs - 1:\n",
    "\n",
    "        print(\"Epoch: {:5d}/{:5d}, total loss = {:.3f}, Fit loss U = {:.3f}, Fit loss F = {:.3f}, KL loss = {:.3f}\".format(i + 1, num_epochs, \n",
    "               loss[i], fit_loss_U_train[i], fit_loss_F_train[i], KL_loss_train[i]))\n",
    "\n",
    "    \n",
    "        lambda1_mus = net.lambda1_mus.item()\n",
    "        lambda1_stds = torch.log(1 + torch.exp(net.lambda1_rhos)).item()\n",
    "        \n",
    "        lambda2_mus = np.exp(net.lambda2_mus.item())\n",
    "        lambda2_stds = torch.log(1 + torch.exp(net.lambda2_rhos)).item()\n",
    "\n",
    "        # noise_f = net.log_noise_u.exp().item()\n",
    "        # noise_u = net.log_noise_u.exp().item()\n",
    "        \n",
    "        samples_25, _ = net.predict(X_u_test_25, 100, net.network)\n",
    "        u_pred_25 = samples_25.mean(axis=0)\n",
    "        error_25 = np.linalg.norm(target_25-u_pred_25, 2)/np.linalg.norm(target_25, 2)\n",
    "\n",
    "        samples_train, _ = net.predict(X_u_train, 100, net.network)\n",
    "        u_pred_train = samples_train.mean(axis=0)\n",
    "        error_train = np.linalg.norm(u_train-u_pred_train, 2)/np.linalg.norm(u_train, 2)\n",
    "\n",
    "\n",
    "        # writer.add_scalars(\"loss/train_test\", {'train':error_train, 'test':error_25}, i)\n",
    "        # writer.add_scalars(\"loss/f_u\", {'noise_f':noise_f, 'noise_u':noise_u}, i)\n",
    "       \n",
    "        print(\"Epoch: {:5d}/{:5d}, lambda1_mu = {:.3f}, lambda2_mu = {:.3f}, lambda1_std = {:.3f}, lambda2_std = {:.3f}\".format(i + 1, num_epochs,\n",
    "                                                                                                                        lambda1_mus, lambda2_mus,\n",
    "                                                                                                                        lambda1_stds, lambda2_stds))\n",
    "        print(\"Epoch: {:5d}/{:5d}, error_25 = {:.5f}, error_train = {:.5f}\".format(i+1, num_epochs, error_25, error_train))\n",
    "        # print(\"Epoch: {:5d}/{:5d}, noise_f = {:.5f}, noise_u = {:.5f}\".format(i+1, num_epochs, noise_f, noise_u))\n",
    "        print()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_u_test_25 = np.hstack([x, 0.25*np.ones_like((x))]); u_test_25 = Exact[25]\n",
    "X_u_test_50 = np.hstack([x, 0.50*np.ones_like((x))]); u_test_50 = Exact[50]\n",
    "X_u_test_75 = np.hstack([x, 0.75*np.ones_like((x))]); u_test_75 = Exact[75]\n",
    "\n",
    "\n",
    "def get_res(X):\n",
    "    samples, noises = net.predict(X, 100, net.network)\n",
    "    u_pred = samples.mean(axis = 0)\n",
    "\n",
    "    aleatoric = (noises**2).mean(axis = 0)**0.5\n",
    "    epistemic = samples.var(axis = 0)**0.5\n",
    "    total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
    "    return u_pred, total_unc\n",
    "\n",
    "# samples_25 = net.predict(X_u_test_25, 100, net.network)\n",
    "# samples_50 = net.predict(X_u_test_50, 100, net.network)\n",
    "# samples_75 = net.predict(X_u_test_75, 100, net.network)\n",
    "\n",
    "# u_pred_25 = samples_25.mean(axis = 0)\n",
    "# u_pred_50 = samples_50.mean(axis = 0)\n",
    "# u_pred_75 = samples_75.mean(axis = 0)\n",
    "\n",
    "u_pred_25, total_unc_25 = get_res(X_u_test_25)\n",
    "u_pred_50, total_unc_50 = get_res(X_u_test_50)\n",
    "u_pred_75, total_unc_75 = get_res(X_u_test_75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (15,4))\n",
    "axs[0].plot(x,u_test_25, 'b-', linewidth = 2, label = 'Exact')\n",
    "axs[0].plot(x,u_pred_25, 'r--', linewidth = 2, label = 'Prediction')\n",
    "plt.fill_between(x, u_pred_25, u_pred_25-total_unc_25, u_pred_25+total_unc_25, color = 'g', alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
    "axs[0].set_xlabel('$x$')\n",
    "axs[0].set_ylabel('$u(t,x)$')\n",
    "axs[0].set_title('$t = 0.25$', fontsize = 10)\n",
    "axs[0].axis('square')\n",
    "#ax.set_xlim([-1.1,1.1])\n",
    "#ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "\n",
    "axs[1].plot(x,u_test_50, 'b-', linewidth = 2, label = 'Exact')\n",
    "axs[1].plot(x,u_pred_50, 'r--', linewidth = 2, label = 'Prediction')\n",
    "axs[1].set_xlabel('$x$')\n",
    "axs[1].set_ylabel('$u(t,x)$')\n",
    "axs[1].set_title('$t = 0.5$', fontsize = 10)\n",
    "axs[1].axis('square')\n",
    "\n",
    "\n",
    "axs[2].plot(x,u_test_75, 'b-', linewidth = 2, label = 'Exact')\n",
    "axs[2].plot(x,u_pred_75, 'r--', linewidth = 2, label = 'Prediction')\n",
    "axs[2].set_xlabel('$x$')\n",
    "axs[2].set_ylabel('$u(t,x)$')\n",
    "axs[2].set_title('$t = 0.75$', fontsize = 10)\n",
    "axs[2].axis('square')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epistemic = samples_25.var(axis = 0)**0.5\n",
    "epistemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.network.named_parameters():\n",
    "    print(name, param)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_gaussian_loss(output, target, sigma, no_dim): # negative\n",
    "#     exponent = -0.5*(target - output)**2/(2*sigma**2) - torch.log(sigma)\n",
    "#     print(exponent.shape)\n",
    "#     return - (exponent).sum()\n",
    "\n",
    "# u_pred, KL_loss_para = net.net_U(X, t)\n",
    "           \n",
    "\n",
    "# fit_loss_U_total = log_gaussian_loss(u_pred, U, net.network.log_noise_u.exp(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.log_noise_u.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca69f467ffdc3dc00e55b12e085102ec88652c053799da0c2cca2d561c3b19a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
