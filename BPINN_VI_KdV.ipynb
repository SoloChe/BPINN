{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import scipy.io\n",
    "from model import BayesLinear_Normalq, BBP_Model\n",
    "from utils import log_gaussian_loss, gaussian, get_kl_Gaussian_divergence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBP_Model_PINN:\n",
    "    def __init__(self, xt_lb, xt_ub, u_lb, u_ub,\n",
    "                 input_dim, output_dim, no_units,\n",
    "                 learn_rate, batch_size, no_batches,\n",
    "                 prior_lambda1, prior_lambda2, num_epochs):\n",
    "\n",
    "        \n",
    "        self.xt_lb = torch.from_numpy(xt_lb).float().to(device)\n",
    "        self.xt_ub = torch.from_numpy(xt_ub).float().to(device)\n",
    "        self.u_lb = torch.from_numpy(u_lb).float().to(device)\n",
    "        self.u_ub = torch.from_numpy(u_ub).float().to(device)\n",
    "\n",
    "\n",
    "        self.learn_rate = learn_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.no_batches = no_batches\n",
    "\n",
    "        self.network = BBP_Model(input_dim = input_dim, output_dim = output_dim,\n",
    "                                    no_units = no_units)\n",
    "\n",
    "\n",
    "        self.prior_lambda1 = prior_lambda1\n",
    "        self.prior_lambda2 = prior_lambda2\n",
    "\n",
    "        self.lambda1_mus = nn.Parameter(torch.Tensor(1).uniform_(0, 2))\n",
    "        self.lambda1_rhos = nn.Parameter(torch.Tensor(1).uniform_(-3, 2))\n",
    "        self.lambda2_mus = nn.Parameter(torch.Tensor(1).uniform_(0, 0.05))\n",
    "        self.lambda2_rhos = nn.Parameter(torch.Tensor(1).uniform_(-3, -2))\n",
    "\n",
    "        self.network.register_parameter('lambda1_mu', self.lambda1_mus)\n",
    "        self.network.register_parameter('lambda2_mu', self.lambda2_mus)\n",
    "        self.network.register_parameter('lambda1_rho', self.lambda1_rhos)\n",
    "        self.network.register_parameter('lambda2_rho', self.lambda2_rhos)\n",
    "\n",
    "        self.network = self.network.to(device)\n",
    "\n",
    "        # self.optimizer = torch.optim.SGD(self.network.parameters(), lr = self.learn_rate)\n",
    "        self.optimizer = torch.optim.AdamW(self.network.parameters(), lr = self.learn_rate)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr = 1e-3, \n",
    "        #                                     steps_per_epoch = no_batches, epochs = num_epochs)\n",
    "        self.loss_func = log_gaussian_loss\n",
    "\n",
    "    def net_U(self, x, t):\n",
    "        xt = torch.cat((x,t), dim=1)\n",
    "        xt = 2*(xt-self.xt_lb)/(self.xt_ub-self.xt_lb) - 1\n",
    "        out, KL_loss = self.network(xt)\n",
    "\n",
    "        u = out[:,0:1]\n",
    "        log_noise_u = out[:,1:2]\n",
    "        \n",
    "        return u, log_noise_u, KL_loss\n",
    "\n",
    "    def net_F(self, x, t, lambda1_sample, lambda2_sample):\n",
    "        lambda_1 = lambda1_sample        \n",
    "        lambda_2 = torch.exp(lambda2_sample)\n",
    "\n",
    "        u, _, _ = self.net_U(x, t)\n",
    "        u = u*(self.u_ub-self.u_lb) + self.u_lb # reverse scaling\n",
    "        # u = u*self.u_std + self.u_mean\n",
    "\n",
    "        u_t = torch.autograd.grad(u, t, torch.ones_like(u),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, torch.ones_like(u),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "        u_xxx = torch.autograd.grad(u_xx, x, torch.ones_like(u_xx),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True)[0]\n",
    "\n",
    "        F = u_t + lambda_1*u*u_x + lambda_2*u_xxx\n",
    "        return F\n",
    "\n",
    "    def fit(self, X, t, U, no_samples):\n",
    "        self.network.train()\n",
    "\n",
    "        U = (U-self.u_lb)/(self.u_ub-self.u_lb) # scaling\n",
    "      \n",
    "\n",
    "        # reset gradient and total loss\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        fit_loss_total = 0\n",
    "        fit_loss_F_total = 0\n",
    "        fit_loss_U_total = 0\n",
    "\n",
    "        for i in range(no_samples):\n",
    "            lambda1_epsilons = self.lambda1_mus.data.new(self.lambda1_mus.size()).normal_()\n",
    "            lambda1_stds = torch.log(1 + torch.exp(self.lambda1_rhos))\n",
    "            lambda2_epsilons = self.lambda2_mus.data.new(self.lambda2_mus.size()).normal_()\n",
    "            lambda2_stds = torch.log(1 + torch.exp(self.lambda2_rhos))\n",
    "\n",
    "            lambda1_sample = self.lambda1_mus + lambda1_epsilons * lambda1_stds\n",
    "            lambda2_sample = self.lambda2_mus + lambda2_epsilons * lambda2_stds\n",
    "\n",
    "            u_pred, log_noise_u, KL_loss_para = self.net_U(X, t)\n",
    "            f_pred = self.net_F(X, t, lambda1_sample, lambda2_sample)\n",
    "\n",
    "            # calculate fit loss based on mean and standard deviation of output\n",
    "            fit_loss_U_total += self.loss_func(u_pred, U, log_noise_u.exp(), self.network.output_dim)\n",
    "            fit_loss_F_total += torch.sum(f_pred**2) ######\n",
    "            # fit_loss_F_total += self.loss_func(f_pred, torch.zeros_like(f_pred), self.network.log_noise_f.exp(), self.network.output_dim)\n",
    "\n",
    "        KL_loss_lambda1 = get_kl_Gaussian_divergence(self.prior_lambda1.mu, self.prior_lambda1.sigma**2, self.lambda1_mus, lambda1_stds**2)\n",
    "        KL_loss_lambda2 = get_kl_Gaussian_divergence(self.prior_lambda2.mu, self.prior_lambda2.sigma**2, self.lambda2_mus, lambda2_stds**2)\n",
    "        KL_loss_total = KL_loss_para + KL_loss_lambda1 + KL_loss_lambda2\n",
    "\n",
    "        # minibatches and KL reweighting\n",
    "        KL_loss_total = KL_loss_total/self.no_batches\n",
    "        total_loss = (KL_loss_total + fit_loss_U_total + fit_loss_F_total) / (no_samples*X.shape[0])\n",
    "        \n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # self.scheduler.step()\n",
    "\n",
    "        return fit_loss_U_total/no_samples, fit_loss_F_total/no_samples, KL_loss_total, total_loss\n",
    "\n",
    "    def predict(self, xt, no_sample, best_net):\n",
    "        xt = torch.tensor(xt, requires_grad=True).float().to(device)\n",
    "        xt = 2*(xt-self.xt_lb)/(self.xt_ub-self.xt_lb) - 1\n",
    "\n",
    "        self.network.eval()\n",
    "        samples = []\n",
    "        noises = []\n",
    "        for i in range(no_sample):\n",
    "            out_pred, _ = best_net(xt)\n",
    "            u_pred = out_pred[:,0:1]\n",
    "            noise_u = out_pred[:,1:2].exp()\n",
    "\n",
    "            u_pred = u_pred*(self.u_ub-self.u_lb) + self.u_lb # reverse scaling\n",
    "            noise_u = noise_u*(self.u_ub-self.u_lb)\n",
    "\n",
    "            samples.append(u_pred.detach().cpu().numpy())\n",
    "            noises.append(noise_u.detach().cpu().numpy())\n",
    "        return np.array(samples), np.array(noises)\n",
    "\n",
    "    def sample_F(self, xt, no_samples = 100):\n",
    "        xt = torch.tensor(xt, requires_grad=True).float().to(device)\n",
    "        xt = 2*(xt-self.xt_lb)/(self.xt_ub-self.xt_lb) - 1\n",
    "        self.network.eval()\n",
    "\n",
    "        samples = []\n",
    "        for _ in range(no_samples):\n",
    "            lambda1_epsilons = self.lambda1_mus.data.new(self.lambda1_mus.size()).normal_()\n",
    "            lambda1_stds = torch.log(1 + torch.exp(self.lambda1_rhos))\n",
    "            lambda2_epsilons = self.lambda2_mus.data.new(self.lambda2_mus.size()).normal_()\n",
    "            lambda2_stds = torch.log(1 + torch.exp(self.lambda2_rhos))\n",
    "\n",
    "            lambda1_sample = self.lambda1_mus + lambda1_epsilons * lambda1_stds\n",
    "            lambda2_sample = self.lambda2_mus + lambda2_epsilons * lambda2_stds\n",
    "            f = self.net_F(xt[:,0:1], xt[:,1:2], lambda1_sample, lambda2_sample)\n",
    "            samples.append(f.detach().cpu().numpy().ravel())\n",
    "        return np.array(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('./Data/KdV.mat')\n",
    "\n",
    "t = data['tt'].flatten()[:,None] # 201 x 1\n",
    "x = data['x'].flatten()[:,None] # 512 x 1\n",
    "Exact_ = np.real(data['uu']).T # 201 x 512\n",
    "\n",
    "noise = 0.1\n",
    "Exact = Exact_ + noise*np.std(Exact_)*np.random.randn(201, 512)\n",
    "\n",
    "X, T = np.meshgrid(x,t) # 201 x 512\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) # 102912 x 2\n",
    "u_star = Exact.flatten()[:,None]  # 102912 x 1\n",
    "\n",
    "# Domain bounds of x, t\n",
    "xt_lb = X_star.min(0)\n",
    "xt_ub = X_star.max(0)\n",
    "\n",
    "# training data\n",
    "N_u = 2000\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "u_min = u_train.min(0)\n",
    "u_max = u_train.max(0)\n",
    "\n",
    "num_epochs, batch_size = 25000, len(X_u_train),\n",
    "\n",
    "net = BBP_Model_PINN(xt_lb, xt_ub, u_min, u_max,\n",
    "                     input_dim = 2, output_dim = 2, no_units = 50, learn_rate = 1e-3,\n",
    "                        batch_size = batch_size, no_batches = 1,\n",
    "                        prior_lambda1 = gaussian(0, 1), prior_lambda2 = gaussian(0, 1), num_epochs = num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_loss_U_train = np.zeros(num_epochs)\n",
    "fit_loss_F_train = np.zeros(num_epochs)\n",
    "KL_loss_train = np.zeros(num_epochs)\n",
    "loss = np.zeros(num_epochs)\n",
    "noise = []\n",
    "\n",
    "\n",
    "X = torch.tensor(X_u_train[:,0:1], requires_grad = True, device = device).float()\n",
    "t = torch.tensor(X_u_train[:,1:2], requires_grad = True, device = device).float()\n",
    "U = torch.tensor(u_train, requires_grad = True, device = device).float()\n",
    "\n",
    "X_u_test_25 = np.hstack([x, 0.25*np.ones_like((x))])\n",
    "target_25 = Exact_[50].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     1/25000, total loss = 1.677, Fit loss U = 2020.866, Fit loss F = 5.173, KL loss = 26574.855\n",
      "Epoch:     1/25000, lambda1_mu = 0.678, lambda2_mu = 1.043, lambda1_std = 1.789, lambda2_std = 0.091\n",
      "Epoch:     1/25000, error_25 = 1.71311, error_train = 1.71122\n",
      "\n",
      "Epoch:   101/25000, total loss = 0.798, Fit loss U = 276.270, Fit loss F = 5.607, KL loss = 26295.662\n",
      "Epoch:   101/25000, lambda1_mu = 0.627, lambda2_mu = 1.030, lambda1_std = 1.727, lambda2_std = 0.100\n",
      "Epoch:   101/25000, error_25 = 1.00004, error_train = 1.00264\n",
      "\n",
      "Epoch:   201/25000, total loss = 0.732, Fit loss U = 148.342, Fit loss F = 7.551, KL loss = 26173.191\n",
      "Epoch:   201/25000, lambda1_mu = 0.612, lambda2_mu = 1.025, lambda1_std = 1.706, lambda2_std = 0.110\n",
      "Epoch:   201/25000, error_25 = 1.00823, error_train = 1.00035\n",
      "\n",
      "Epoch:   301/25000, total loss = 0.798, Fit loss U = 290.179, Fit loss F = 3.261, KL loss = 26050.551\n",
      "Epoch:   301/25000, lambda1_mu = 0.593, lambda2_mu = 1.017, lambda1_std = 1.684, lambda2_std = 0.120\n",
      "Epoch:   301/25000, error_25 = 1.02424, error_train = 0.99982\n",
      "\n",
      "Epoch:   401/25000, total loss = 0.598, Fit loss U = -103.192, Fit loss F = 3.593, KL loss = 25923.396\n",
      "Epoch:   401/25000, lambda1_mu = 0.576, lambda2_mu = 1.002, lambda1_std = 1.656, lambda2_std = 0.132\n",
      "Epoch:   401/25000, error_25 = 1.00588, error_train = 1.00061\n",
      "\n",
      "Epoch:   501/25000, total loss = 0.588, Fit loss U = -131.553, Fit loss F = 17.358, KL loss = 25792.475\n",
      "Epoch:   501/25000, lambda1_mu = 0.542, lambda2_mu = 0.929, lambda1_std = 1.609, lambda2_std = 0.141\n",
      "Epoch:   501/25000, error_25 = 0.99786, error_train = 1.00220\n",
      "\n",
      "Epoch:   601/25000, total loss = 0.619, Fit loss U = -54.786, Fit loss F = 8.845, KL loss = 25659.842\n",
      "Epoch:   601/25000, lambda1_mu = 0.464, lambda2_mu = 0.819, lambda1_std = 1.507, lambda2_std = 0.142\n",
      "Epoch:   601/25000, error_25 = 0.99856, error_train = 0.99884\n",
      "\n",
      "Epoch:   701/25000, total loss = 0.546, Fit loss U = -190.216, Fit loss F = 6.793, KL loss = 25526.574\n",
      "Epoch:   701/25000, lambda1_mu = 0.424, lambda2_mu = 0.752, lambda1_std = 1.427, lambda2_std = 0.139\n",
      "Epoch:   701/25000, error_25 = 0.99859, error_train = 0.99887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment = '_KdV')\n",
    "\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    EU, EF, KL_loss, total_loss = net.fit(X, t, U, no_samples = 20)\n",
    "    \n",
    "    fit_loss_U_train[i] = EU.item()\n",
    "    fit_loss_F_train[i] = EF.item()\n",
    "    KL_loss_train[i] = KL_loss.item()\n",
    "    loss[i] = total_loss.item()\n",
    "\n",
    "    writer.add_scalar(\"loss/total_loss\", loss[i], i)\n",
    "    writer.add_scalar(\"loss/U_loss\", fit_loss_U_train[i], i)\n",
    "    writer.add_scalar(\"loss/F_loss\", fit_loss_F_train[i], i)\n",
    "    writer.add_scalar(\"loss/KL_loss\", KL_loss_train[i], i)\n",
    "    \n",
    "\n",
    "    if i % 2000 == 0:\n",
    "        F_test = net.sample_F(X_u_test_25)\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(20, 8))\n",
    "        axs[0,0].hist(F_test[:,0])\n",
    "        axs[0,1].hist(F_test[:,100])\n",
    "        axs[1,0].hist(F_test[:,150])\n",
    "        axs[1,1].hist(F_test[:,255])\n",
    "        plt.savefig('./plots/kdv_epoch{}.tiff'.format(i))\n",
    "\n",
    "\n",
    "    if i % 100 == 0 or i == num_epochs - 1:\n",
    "\n",
    "        print(\"Epoch: {:5d}/{:5d}, total loss = {:.3f}, Fit loss U = {:.3f}, Fit loss F = {:.3f}, KL loss = {:.3f}\".format(i + 1, num_epochs, \n",
    "               loss[i], fit_loss_U_train[i], fit_loss_F_train[i], KL_loss_train[i]))\n",
    "\n",
    "    \n",
    "        lambda1_mus = net.lambda1_mus.item()\n",
    "        lambda1_stds = torch.log(1 + torch.exp(net.lambda1_rhos)).item()\n",
    "        \n",
    "        lambda2_mus = np.exp(net.lambda2_mus.item())\n",
    "        lambda2_stds = torch.log(1 + torch.exp(net.lambda2_rhos)).item()\n",
    "        \n",
    "        samples_25, _ = net.predict(X_u_test_25, 100, net.network)\n",
    "        u_pred_25 = samples_25.mean(axis=0)\n",
    "        error_25 = np.linalg.norm(target_25-u_pred_25, 2)/np.linalg.norm(target_25, 2)\n",
    "\n",
    "        samples_train, _ = net.predict(X_u_train, 100, net.network)\n",
    "        u_pred_train = samples_train.mean(axis=0)\n",
    "        error_train = np.linalg.norm(u_train-u_pred_train, 2)/np.linalg.norm(u_train, 2)\n",
    "\n",
    "\n",
    "        # writer.add_scalars(\"loss/train_test\", {'train':error_train, 'test':error_25}, i)\n",
    "        # writer.add_scalars(\"loss/f_u\", {'noise_f':noise_f, 'noise_u':noise_u}, i)\n",
    "       \n",
    "        print(\"Epoch: {:5d}/{:5d}, lambda1_mu = {:.3f}, lambda2_mu = {:.3f}, lambda1_std = {:.3f}, lambda2_std = {:.3f}\".format(i + 1, num_epochs,\n",
    "                                                                                                                        lambda1_mus, lambda2_mus,\n",
    "                                                                                                                        lambda1_stds, lambda2_stds))\n",
    "        print(\"Epoch: {:5d}/{:5d}, error_25 = {:.5f}, error_train = {:.5f}\".format(i+1, num_epochs, error_25, error_train))\n",
    "        # print(\"Epoch: {:5d}/{:5d}, noise_f = {:.5f}, noise_u = {:.5f}\".format(i+1, num_epochs, noise_f, noise_u))\n",
    "        print()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = data['x'].flatten()[:,None]\n",
    "X_u_test_25 = np.hstack([x, 0.25*np.ones_like((x))]); u_test_25 = Exact[50]; u_mean_25 = Exact_[50]\n",
    "X_u_test_50 = np.hstack([x, 0.50*np.ones_like((x))]); u_test_50 = Exact[100]; u_mean_50 = Exact_[100]\n",
    "X_u_test_75 = np.hstack([x, 0.75*np.ones_like((x))]); u_test_75 = Exact[150]; u_mean_75 = Exact_[150]\n",
    "\n",
    "\n",
    "def get_res(X):\n",
    "    samples, noises = net.predict(X, 100, net.network)\n",
    "    u_pred = samples.mean(axis = 0)\n",
    "\n",
    "    aleatoric = (noises**2).mean(axis = 0)**0.5\n",
    "    epistemic = samples.var(axis = 0)**0.5\n",
    "    total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
    "    return u_pred.ravel(), aleatoric.ravel(), epistemic.ravel(), total_unc.ravel()\n",
    "\n",
    "\n",
    "x = x.ravel()\n",
    "u_pred_25, ale_25, epi_25, total_unc_25 = get_res(X_u_test_25)\n",
    "u_pred_50, ale_50, epi_50, total_unc_50 = get_res(X_u_test_50)\n",
    "u_pred_75, ale_75, epi_75, total_unc_75 = get_res(X_u_test_75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (15,4))\n",
    "axs[0].scatter(x, u_test_25, s = 10, marker = 'x', color = 'black', alpha = 0.5, label = 'Exact')\n",
    "axs[0].plot(x, u_mean_25, 'b-', linewidth = 2, label = 'Prediction')\n",
    "axs[0].plot(x, u_pred_25, 'r--', linewidth = 2, label = 'Prediction')\n",
    "# axs[0].fill_between(x, u_pred_25-epi_25, u_pred_25+epi_25, color = 'g', alpha = 0.5, label = 'Epistemic + Aleatoric')\n",
    "# axs[0].fill_between(x, u_pred_25-ale_25, u_pred_25+ale_25, color = 'g', alpha = 0.5, label = 'Epistemic + Aleatoric')\n",
    "axs[0].fill_between(x, u_pred_25-2*total_unc_25, u_pred_25+2*total_unc_25, color = 'g', alpha = 0.5, label = 'Epistemic + Aleatoric')\n",
    "axs[0].set_xlabel('$x$')\n",
    "axs[0].set_ylabel('$u(t,x)$')\n",
    "axs[0].set_title('$t = 0.25$', fontsize = 10)\n",
    "# axs[0].set_xlim([-1.1,1.1])\n",
    "# ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "\n",
    "axs[1].scatter(x, u_test_50, s = 10, marker = 'x', color = 'black', alpha = 0.5, label = 'Exact')\n",
    "axs[1].plot(x, u_mean_50, 'b-', linewidth = 2, label = 'Prediction')\n",
    "axs[1].plot(x, u_pred_50, 'r--', linewidth = 2, label = 'Prediction')\n",
    "axs[1].fill_between(x, u_pred_50-2*total_unc_50, u_pred_50+2*total_unc_50, color = 'g', alpha = 0.5, label = 'Epistemic + Aleatoric')\n",
    "axs[1].set_xlabel('$x$')\n",
    "axs[1].set_ylabel('$u(t,x)$')\n",
    "axs[1].set_title('$t = 0.5$', fontsize = 10)\n",
    "\n",
    "\n",
    "axs[2].scatter(x, u_test_75, s = 10, marker = 'x', color = 'black', alpha = 0.5, label = 'Exact')\n",
    "axs[2].plot(x, u_mean_75, 'b-', linewidth = 2, label = 'Prediction')\n",
    "axs[2].plot(x, u_pred_75, 'r--', linewidth = 2, label = 'Prediction')\n",
    "axs[2].fill_between(x, u_pred_75-2*total_unc_75, u_pred_75+2*total_unc_75, color = 'g', alpha = 0.5, label = 'Epistemic + Aleatoric')\n",
    "axs[2].set_xlabel('$x$')\n",
    "axs[2].set_ylabel('$u(t,x)$')\n",
    "axs[2].set_title('$t = 0.75$', fontsize = 10)\n",
    "\n",
    "\n",
    "plt.savefig('./plots/final_prediction_kdv.tiff')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca69f467ffdc3dc00e55b12e085102ec88652c053799da0c2cca2d561c3b19a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
